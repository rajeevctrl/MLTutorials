{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers,models,optimizers\n",
    "from keras.layers import Input,Conv2D,Dense, Reshape,Layer,Lambda\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "import keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(output_vector, axis=-1):\n",
    "    norm = tf.reduce_sum(tf.square(output_vector), axis, keep_dims=True)\n",
    "    return output_vector * norm / ((1 + norm) * tf.sqrt(norm + 1.0e-10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskingLayer(Layer):\n",
    "    \"\"\"\n",
    "    Mask a Tensor with shape=[None, d1, d2] by the max value in axis=1.\n",
    "    Output shape: [None, d2]\n",
    "    \"\"\"\n",
    "    def call(self,inputs,**kwargs):\n",
    "        # inputs is a list with [inputs_, mask]. Where inputs_ is output of digit Capsule layer with shape\n",
    "        # [batch_size, num_classes, dim_vector] and mask is actual one hot encoded Y_train with shape\n",
    "        # [batch_size, num_classes]. This function computes dot product of actual class one-hot vector with\n",
    "        # [num_classes, dim_vector] shaped capsule outputs\n",
    "        \n",
    "        # So for each input we will be having num_classes output vectors each of size dim_vector. This function\n",
    "        # will basically return the output vector pertaining to actual class. Thus output shape of this function\n",
    "        # will be [batch_size, dim_vector]\n",
    "        input_, mask = inputs\n",
    "        masked_inputs = K.batch_dot(input_,mask, axes=1) # masked_inputs.shape = [batch_size, dim_vector]\n",
    "        return masked_inputs\n",
    "    \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        *_,output_shape = input_shape[0]\n",
    "        return (None,output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrimaryCapsule(inputs,dim_vector,n_channels,n_kernel_size,n_stride,padding=\"valid\"):\n",
    "    \"\"\"\n",
    "    Converts [batch_size,width,height,num_filters] shaped output of a regular Conv2D layer to output of\n",
    "    Capsule Conv2D layer of shape [batch_size,width,height,n_channels*dim_vector] then reshapes this output\n",
    "    to shape [batch_size, width*height*n_channels, dim_vectors] i.e. flattens the tensor keeping the dimension\n",
    "    of capsule vector unchanged.\n",
    "    \"\"\"\n",
    "    # conv_output.shape = [batch_size,width,height,n_channels*dim_vector]\n",
    "    conv_output = Conv2D(filters = n_channels * dim_vector,\n",
    "                  kernel_size = n_kernel_size,\n",
    "                  strides = n_stride,\n",
    "                  padding = padding,\n",
    "                  name = \"primary_capsule_conv2d_layer\")(inputs)\n",
    "    # Reshape conv_output by merging all dimensions except dim_vector dimension.\n",
    "    # reshaped_output.shape = [batch_size, width*height*n_channels, dim_vector]\n",
    "    reshaped_output = Reshape(target_shape = (-1,dim_vector),\n",
    "                             name = \"primary_capsule_reshape_layer\")(conv_output)\n",
    "    # Apply squash function to reshaped_output\n",
    "    # reshaped_output.shape = [batch_size, widht*height*n_channels, dim_vector]\n",
    "    squashed_output = Lambda(squash, name=\"primary_capsule_squash_layer\")(reshaped_output)\n",
    "    return squashed_output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleLayer(Layer):\n",
    "    \"\"\"\n",
    "    Takes vector outputs of Primary capsule layer at each neuron and each neuron computes an output vector.\n",
    "    Input shape to each neuron of current layer is [batch_size, prev_layer_num_capsules, prev_layer_dim_vector]\n",
    "    Output shape of single neuron of current layer is [batch_size,dim_vector]\n",
    "    Output shape for all neurons of current layers is [batch_size, num_capsules, dim_vector]\n",
    "    \n",
    "    This is like as Dense layer. Only difference is that each neuron of Dense layer takes \n",
    "    [batch_size, prev_layer_num_neurons] shaped input and produces scalar output. While each capsule of\n",
    "    CapsuleLayer takes [batch_size, prev_layer_num_capsules, prev_layer_dim_vector] shaped input and produces\n",
    "    [batch_size, dim_vector] shaped output.\n",
    "    \"\"\"\n",
    "    def __init__(self,n_capsule, dim_vector, n_routing, **kwargs):\n",
    "        super(CapsuleLayer,self).__init__(**kwargs)\n",
    "        self.n_capsule = n_capsule\n",
    "        self.dim_vector = dim_vector\n",
    "        self.n_routing = n_routing\n",
    "        self.kernel_initializer = initializers.get(\"he_normal\")\n",
    "        self.bias_initializer = initializers.get(\"zeros\")\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        _ , self.input_n_capsule, self.input_dim_vector, *_ = input_shape\n",
    "        # Initialize weights of this layer.\n",
    "        self.W = self.add_weight(shape = [self.input_n_capsule, self.n_capsule, self.input_dim_vector, self.dim_vector],\n",
    "                                initializer = self.kernel_initializer,\n",
    "                                name = \"W\")\n",
    "        # Initialize bias of this layer.\n",
    "        # There is a bias associated with output of each input_n_capsule to each n_capsule. Thus\n",
    "        # actual shape of bias is [input_n_capsule, n_capsule] however to make it similar to W\n",
    "        # add dummy dimensions for batch_size (dim0), input_dim_vector (dim3) and dim_vector (dim4) \n",
    "        self.bias = self.add_weight(shape=[1, self.input_n_capsule, self.n_capsule, 1, 1], \n",
    "                                    initializer=self.bias_initializer, \n",
    "                                    name='bias', \n",
    "                                    trainable=False)\n",
    "        self.built = True\n",
    "        \n",
    "    def call(self,inputs, training = None):\n",
    "        # inputs.shape = [batch_size, input_num_capsule, input_dim_vector]\n",
    "        # However dimensions of inputs should match with W (excluding batch_size) thus add dummy\n",
    "        # dimensions for num_capsule and input_dim_vector.\n",
    "        # Expand dims to [batch_size, input_num_capsule, 1, 1 ,input_dim_vector]\n",
    "        input_expand = K.expand_dims(tf.expand_dims(inputs,axis=2),2)\n",
    "        \n",
    "        # Now same input_expand will be send to each capsule of current layer i.e. to n_capsule. So\n",
    "        # replicate input_expand n_capsule times for n_capsule dimension (dim2).\n",
    "        input_tiles = K.tile(input_expand, [1,1,self.n_capsule,1,1])\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute dot product of W and input. Each vector of previous layer is routed to each neuron of current layer.\n",
    "        Each prev layer vector 'u' is represented by last two dimensions i.e. [1, input_dim_vector]. This vector will\n",
    "        be routed to each neuron of next layer and its dimensionality will be changed to [1, dim_vector]. For this\n",
    "        we have a transformation matrix w_expand of shape [:,:,:, input_dim_vector, dim_vector]. For this specific\n",
    "        vector consider this transformation sub-matrix to be of shape [input_dim_vector,dim_vector]. Now a single\n",
    "        vector u.shape[1,input_dim_vector] is multiplied with transformation matrix \n",
    "        w.shape[input_dim_vector, dim_vector] to generate a new vector u_hat.shape[1,dim_vector]. Now this generated\n",
    "        vector u_hat is routed to all neurons of current layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute dot product of weight matrix with output vector of capsule of previous layer to generate\n",
    "        # a prediction vector for each capsule of previous layer.\n",
    "        \n",
    "        # Same W is used for each datapoint of batch so replicate W batch_size times and add extra dim0 to W\n",
    "        # to represent batch_size.\n",
    "        # w_expand.shape = [batch_size, input_n_capsule, n_capsule, input_dim_vector, dim_vector]\n",
    "        ### w_expand = K.expand_dims(self.W,axis=0)\n",
    "        # Replicate w_expand batch_size times.\n",
    "        ### w_tiles = K.tile(w_expand, [self.batch_size,1,1,1,1])\n",
    "        # Now input and W have same shape, so do K.batch_dot of w_tiles and input_tiles along dim4 and dim3.\n",
    "        # So do matrix multiplication of input_tiles * w_tiles using shapes \n",
    "        # [:,:,:, 1,input_dim_vector] * [:,:,:, input_dim_vector, dim_vector] = [:,:,:, 1, dim_vector]\n",
    "        # This multiplication transforms each input_vector of dimension input_dim_vector to dimension dim_vector.\n",
    "        # axes=[4,3] means do dot product on dim4 of input_tiles and dim3 of w_tiles.\n",
    "        # inputs_hat.shape = [batch_size, input_n_capsule, n_capsule, 1, dim_vector] i.e. input vectors\n",
    "        # u of dimensionality 'input_dim_vector' are transformed to output vectors u_hat of dimensionality\n",
    "        # 'dim_vector'\n",
    "        ### inputs_hat = K.batch_dot(input_tiles,w_tiles, axes=[4,3])\n",
    "        \n",
    "        \"\"\"\n",
    "        Above computation is just and explanation of what is happening in below line. For above computation\n",
    "        we explicitly need batch_size that is not available (we have to explicitly pass it). So a better way\n",
    "        is to share the weights across computations using below line of code.\n",
    "        \"\"\"\n",
    "        inputs_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]), \n",
    "                             elems=input_tiles, \n",
    "                             initializer=K.zeros( [self.input_n_capsule, self.n_capsule, 1, self.dim_vector]))\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Now that we have u_hats (input_hats) we could proceed with routing algorithm.\n",
    "        \"\"\"\n",
    "        for i in range(self.n_routing):\n",
    "            # Each of output of prev layer will be weighted using a 'c' weight and forwarded for current layer\n",
    "            # capsule. So 'c' weights are to be computed for each neuron of prev layer.\n",
    "            # Initial values of 'c' will be same so initialize them with softmax(bias) as initial values of bias is\n",
    "            # 0 matrix.\n",
    "            # Each neuron of prev layer has number of biases of shape [n_capsule,1,1]\n",
    "            c = tf.nn.softmax(self.bias, dim=2) # c.shape = [num_batches,input_n_capsule,n_capsule,1,1]\n",
    "            \n",
    "            # Now considering a single neuron of current layer, take output vectors from all neurons from prev\n",
    "            # layer, multiply them with respective 'c' and add all weighted vectors together.\n",
    "            weighted_sum_outputs = tf.reduce_sum(c * inputs_hat, axis=1,keep_dims=True)\n",
    "            \n",
    "            # Now squash weighted sum vector such that its norm is 1.\n",
    "            # outputs.shape = [batch_size, 1, n_capsule, 1, dim_vector]\n",
    "            outputs = squash(weighted_sum_outputs)\n",
    "            \n",
    "            # Keep on updating 'c' weights till second last iteration.\n",
    "            if i != self.n_routing - 1:\n",
    "                # compute similarity between  predicted and actual vector.\n",
    "                similarity = inputs_hat * outputs\n",
    "                self.bias += tf.reduce_sum(similarity,axis=-1, keep_dims=True)\n",
    "                \n",
    "            # Reshape output to shape [batch_size, n_capsule, dim_vector] i.e. for each input datapoint\n",
    "            # each of n_capsule generates dim_vector dimensional output vector.\n",
    "            reshaped_output = tf.reshape(outputs,[-1,self.n_capsule, self.dim_vector])\n",
    "            \n",
    "            return reshaped_output\n",
    "        \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        \"\"\"\n",
    "        Computes output shape for this layer.\n",
    "        \"\"\"\n",
    "        return (None,self.n_capsule, self.dim_vector)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LengthLayer(Layer):\n",
    "    \"\"\"\n",
    "    Computes L2 length for each vector\n",
    "    Output shape is [batch_size,1]\n",
    "    \"\"\"\n",
    "    def call(self,inputs,**kwargs):\n",
    "        # square each input value.\n",
    "        squared_inputs = tf.square(inputs) \n",
    "        # Compute sum of squares for each vector and remove all reduced dimensions.\n",
    "        sum_of_squares = tf.reduce_sum(squared_inputs, axis=-1, keep_dims=False)\n",
    "        return tf.sqrt(sum_of_squares)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        *output_shape, _ = input_shape\n",
    "        return tuple(output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_loss(y_ground_truth, y_prediction):\n",
    "    _m_plus = 0.9\n",
    "    _m_minus = 0.1\n",
    "    _lambda = 0.5\n",
    "    L = y_ground_truth * tf.square(tf.maximum(0., _m_plus - y_prediction)) + _lambda * ( 1 - y_ground_truth) * tf.square(tf.maximum(0., y_prediction - _m_minus))\n",
    "    return tf.reduce_mean(tf.reduce_sum(L, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [28, 28, 1]\n",
    "n_class = 10\n",
    "n_routing = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X, Y: (70000, 28, 28, 1) (70000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Create training data.\n",
    "\n",
    "(X_train,Y_train),(X_test,Y_test) = mnist.load_data()\n",
    "X_train = X_train.astype(np.float32)[:,:,:,None] / 255.0\n",
    "X_test = X_test.astype(np.float32)[:,:,:,None] / 255.0\n",
    "\n",
    "Y_train = to_categorical(Y_train.astype(np.float32),num_classes=n_class)\n",
    "Y_test = to_categorical(Y_test.astype(np.float32),num_classes=n_class)\n",
    "\n",
    "# Concatenate train and test data.\n",
    "X = np.concatenate((X_train,X_test), axis=0)\n",
    "Y = np.concatenate((Y_train,Y_test), axis=0)\n",
    "\n",
    "print(\"X, Y:\",X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 10)\n"
     ]
    }
   ],
   "source": [
    "# Create encoder part of network. This part computes probability of each digit class.\n",
    "\n",
    "x = Input(shape = input_shape)\n",
    "# First normal convolution layer.\n",
    "# conv1.shape = [batch_size,width,height,channels]\n",
    "conv1 = Conv2D(filters=256, kernel_size=9, strides=1, padding=\"valid\", activation=\"relu\", name=\"conv1\")(x)\n",
    "\n",
    "\n",
    "# Take output of Conv2D layer and convert it to primary capsules.\n",
    "# primary_capsule.shape = [batch_size,width,height,n_channels*dim_vector]\n",
    "primary_capsule = PrimaryCapsule(inputs = conv1, dim_vector = 8, n_channels = 32,\n",
    "                                n_kernel_size=9, n_stride=2)\n",
    "\n",
    "# Now give primary_capsules as input to capsule layer which takes these vectors as input and performs routing\n",
    "# and generates new vectors for next layer.\n",
    "# digit_capsule.shape = [batch_size, n_capsule, dim_vector]\n",
    "digit_capsule = CapsuleLayer(n_capsule = n_class, dim_vector = 16, n_routing = n_routing, name=\"digit_capsule\")(primary_capsule)\n",
    "\n",
    "# Now compute length of each output vector from previous layer. This will act as final class probability.\n",
    "# output_capsule.shape = [batch_size, num_classes]\n",
    "output_capsule = LengthLayer(name=\"output_capsule\")(digit_capsule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decoder part. This is added after encoder part to regenerate image.\n",
    "\n",
    "mask_input = Input(shape = (n_class,))\n",
    "\n",
    "# mask.shape = [batch_size,dim_vector]\n",
    "mask = MaskingLayer()([digit_capsule,mask_input]) # Get output vectors pertaining to actual class of digit.\n",
    "\n",
    "# A group of dense layers.\n",
    "dec = Dense(512, activation='relu')(mask)\n",
    "dec = Dense(1024, activation='relu')(dec)\n",
    "dec = Dense(784, activation='sigmoid')(dec)\n",
    "dec = Reshape(input_shape)(dec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 20, 20, 256)  20992       input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "primary_capsule_conv2d_layer (C (None, 6, 6, 256)    5308672     conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primary_capsule_reshape_layer ( (None, 1152, 8)      0           primary_capsule_conv2d_layer[0][0\n",
      "__________________________________________________________________________________________________\n",
      "primary_capsule_squash_layer (L (None, 1152, 8)      0           primary_capsule_reshape_layer[0][\n",
      "__________________________________________________________________________________________________\n",
      "digit_capsule (CapsuleLayer)    (None, 10, 16)       1486080     primary_capsule_squash_layer[0][0\n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_layer_4 (MaskingLayer)  (None, 16)           0           digit_capsule[0][0]              \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          8704        masking_layer_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         525312      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 784)          803600      dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_capsule (LengthLayer)    (None, 10)           0           digit_capsule[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 28, 28, 1)    0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,153,360\n",
      "Trainable params: 8,141,840\n",
      "Non-trainable params: 11,520\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model.\n",
    "\n",
    "model = Model([x,mask_input],[output_capsule,dec])\n",
    "model.compile(optimizer=\"adam\",loss=[margin_loss,\"mae\"],metrics=[margin_loss,\"mae\",\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 56000 samples, validate on 14000 samples\n",
      "Epoch 1/3\n",
      "56000/56000 [==============================] - 2913s 52ms/step - loss: 0.2166 - output_capsule_loss: 0.0869 - reshape_1_loss: 0.1297 - output_capsule_margin_loss: 0.0869 - output_capsule_mean_absolute_error: 0.1010 - output_capsule_acc: 0.9165 - reshape_1_margin_loss: 2.1386 - reshape_1_mean_absolute_error: 0.1297 - reshape_1_acc: 0.8010 - val_loss: 0.1265 - val_output_capsule_loss: 0.0225 - val_reshape_1_loss: 0.1040 - val_output_capsule_margin_loss: 0.0225 - val_output_capsule_mean_absolute_error: 0.0737 - val_output_capsule_acc: 0.9841 - val_reshape_1_margin_loss: 2.0126 - val_reshape_1_mean_absolute_error: 0.1040 - val_reshape_1_acc: 0.8032\n",
      "Epoch 2/3\n",
      "56000/56000 [==============================] - 2775s 50ms/step - loss: 0.1154 - output_capsule_loss: 0.0201 - reshape_1_loss: 0.0953 - output_capsule_margin_loss: 0.0201 - output_capsule_mean_absolute_error: 0.0714 - output_capsule_acc: 0.9869 - reshape_1_margin_loss: 1.7867 - reshape_1_mean_absolute_error: 0.0953 - reshape_1_acc: 0.8040 - val_loss: 0.1010 - val_output_capsule_loss: 0.0154 - val_reshape_1_loss: 0.0855 - val_output_capsule_margin_loss: 0.0154 - val_output_capsule_mean_absolute_error: 0.0685 - val_output_capsule_acc: 0.9898 - val_reshape_1_margin_loss: 1.5349 - val_reshape_1_mean_absolute_error: 0.0855 - val_reshape_1_acc: 0.8044\n",
      "Epoch 3/3\n",
      "19328/56000 [=========>....................] - ETA: 31:06 - loss: 0.0982 - output_capsule_loss: 0.0150 - reshape_1_loss: 0.0831 - output_capsule_margin_loss: 0.0150 - output_capsule_mean_absolute_error: 0.0692 - output_capsule_acc: 0.9902 - reshape_1_margin_loss: 1.4761 - reshape_1_mean_absolute_error: 0.0831 - reshape_1_acc: 0.8050"
     ]
    }
   ],
   "source": [
    "# Train capsnet model.\n",
    "\n",
    "hist = model.fit([X, Y], [Y, X], batch_size=128, epochs=3, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
